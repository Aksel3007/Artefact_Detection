{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import EEGDataset\n",
    "from torch.utils.data import random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set\n",
      "../data/study_1A_mat_simple/S_01/night_1/artefact_annotations.npy\n",
      "../data/study_1A_mat_simple/S_01/night_1/EEG_raw_250hz.npy\n",
      "../data/study_1A_mat_simple/S_01/night_2/artefact_annotations.npy\n",
      "../data/study_1A_mat_simple/S_01/night_2/EEG_raw_250hz.npy\n",
      "../data/study_1A_mat_simple/S_01/night_3/artefact_annotations.npy\n",
      "../data/study_1A_mat_simple/S_01/night_3/EEG_raw_250hz.npy\n",
      "../data/study_1A_mat_simple/S_01/night_4/artefact_annotations.npy\n",
      "../data/study_1A_mat_simple/S_01/night_4/EEG_raw_250hz.npy\n",
      "../data/study_1A_mat_simple/S_02/night_1/artefact_annotations.npy\n",
      "../data/study_1A_mat_simple/S_02/night_1/EEG_raw_250hz.npy\n",
      "../data/study_1A_mat_simple/S_02/night_2/artefact_annotations.npy\n",
      "../data/study_1A_mat_simple/S_02/night_2/EEG_raw_250hz.npy\n",
      "../data/study_1A_mat_simple/S_02/night_3/artefact_annotations.npy\n",
      "../data/study_1A_mat_simple/S_02/night_3/EEG_raw_250hz.npy\n",
      "../data/study_1A_mat_simple/S_02/night_4/artefact_annotations.npy\n",
      "../data/study_1A_mat_simple/S_02/night_4/EEG_raw_250hz.npy\n",
      "../data/study_1A_mat_simple/S_03/night_1/artefact_annotations.npy\n",
      "../data/study_1A_mat_simple/S_03/night_1/EEG_raw_250hz.npy\n",
      "../data/study_1A_mat_simple/S_03/night_2/artefact_annotations.npy\n",
      "Lables for night 0 loaded\n",
      "../data/study_1A_mat_simple/S_03/night_2/EEG_raw_250hz.npy\n",
      "Night 0 data loaded\n",
      "../data/study_1A_mat_simple/S_03/night_3/artefact_annotations.npy\n",
      "Lables for night 1 loaded\n",
      "../data/study_1A_mat_simple/S_03/night_3/EEG_raw_250hz.npy\n",
      "Night 1 data loaded\n",
      "Training set\n",
      "../data/study_1A_mat_simple/S_01/night_1/artefact_annotations.npy\n",
      "Lables for night 0 loaded\n",
      "../data/study_1A_mat_simple/S_01/night_1/EEG_raw_250hz.npy\n",
      "Night 0 data loaded\n",
      "../data/study_1A_mat_simple/S_01/night_2/artefact_annotations.npy\n",
      "Lables for night 1 loaded\n",
      "../data/study_1A_mat_simple/S_01/night_2/EEG_raw_250hz.npy\n",
      "Night 1 data loaded\n",
      "../data/study_1A_mat_simple/S_01/night_3/artefact_annotations.npy\n",
      "Lables for night 2 loaded\n",
      "../data/study_1A_mat_simple/S_01/night_3/EEG_raw_250hz.npy\n",
      "Night 2 data loaded\n",
      "../data/study_1A_mat_simple/S_01/night_4/artefact_annotations.npy\n",
      "Lables for night 3 loaded\n",
      "../data/study_1A_mat_simple/S_01/night_4/EEG_raw_250hz.npy\n",
      "Night 3 data loaded\n",
      "../data/study_1A_mat_simple/S_02/night_1/artefact_annotations.npy\n",
      "Lables for night 4 loaded\n",
      "../data/study_1A_mat_simple/S_02/night_1/EEG_raw_250hz.npy\n",
      "Night 4 data loaded\n",
      "../data/study_1A_mat_simple/S_02/night_2/artefact_annotations.npy\n",
      "Lables for night 5 loaded\n",
      "../data/study_1A_mat_simple/S_02/night_2/EEG_raw_250hz.npy\n",
      "Night 5 data loaded\n",
      "../data/study_1A_mat_simple/S_02/night_3/artefact_annotations.npy\n",
      "Lables for night 6 loaded\n",
      "../data/study_1A_mat_simple/S_02/night_3/EEG_raw_250hz.npy\n",
      "Night 6 data loaded\n",
      "../data/study_1A_mat_simple/S_02/night_4/artefact_annotations.npy\n",
      "Lables for night 7 loaded\n",
      "../data/study_1A_mat_simple/S_02/night_4/EEG_raw_250hz.npy\n",
      "Night 7 data loaded\n",
      "../data/study_1A_mat_simple/S_03/night_1/artefact_annotations.npy\n",
      "Lables for night 8 loaded\n",
      "../data/study_1A_mat_simple/S_03/night_1/EEG_raw_250hz.npy\n",
      "Night 8 data loaded\n"
     ]
    }
   ],
   "source": [
    "# load in the dataset\n",
    "\n",
    "#raw_data_dir = '//uni.au.dk/dfs/Tech_EarEEG/Students/RD2022_Artefact_AkselStark/data/1A/study_1A_mat_simple'\n",
    "raw_data_dir = '../data'\n",
    "\n",
    "trainingNights = 9\n",
    "testNights = 2\n",
    "\n",
    "print(\"Test set\")\n",
    "ds2 = EEGDataset(raw_data_dir,testNights, 250, skips = trainingNights)\n",
    "\n",
    "print(\"Training set\")\n",
    "ds1 = EEGDataset(raw_data_dir,trainingNights, 250, skips = 0) #Instantiate a dataset using the directory of data, amount of night to include and amount of samples in a segment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug\n"
     ]
    }
   ],
   "source": [
    "#Calculate class imbalance\n",
    "balancing_dataloader = DataLoader(ds1, drop_last = True) # Drop_last, to avoid incomplete batches, which won't work with weighted loss\n",
    "artefacts = 0\n",
    "good_samples = 0\n",
    "for batch, (X, y) in enumerate(balancing_dataloader):\n",
    "    if y == 1:\n",
    "        artefacts += 1\n",
    "    else:\n",
    "        good_samples += 1\n",
    "    if batch > 100000:\n",
    "        break\n",
    "\n",
    "class_ratio = good_samples/artefacts\n",
    "print(\"debug\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' #Check for cuda \n",
    "print(f'Using {device} device')\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        #self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.BatchNorm1d(250),\n",
    "            nn.Linear(250, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid(), \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model.to(device)\n",
    "\n",
    "#pos_weight: amount of positive examples compared to negative examples. Calculate as: negative_examples/positive_examples\n",
    "loss = nn.BCEWithLogitsLoss(pos_weight = class_ratio*torch.ones([batch_size]).to(device)) \n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)# Reshape to 1 dimension if using binary classification, otherwise keep dimensions from model output\n",
    "        pred = pred.reshape(-1)\n",
    "        pred = pred.to(device)\n",
    "        yFloat = y.type(torch.FloatTensor).to(device)\n",
    "        \n",
    "        loss = loss_fn(pred, yFloat)\n",
    "        \n",
    "\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            #print(\"Predicted values:\")\n",
    "            #print(pred)\n",
    "            #print(\"Actual values:\")\n",
    "            #print(yFloat)\n",
    "\n",
    "\n",
    "\n",
    "def test_loop(dataloader_test, model, loss_fn, test_set = True):\n",
    "    size = len(dataloader_test.dataset)\n",
    "    num_batches = len(dataloader_test)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader_test:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X).reshape(-1).to(device) # Reshape to 1 dimension if using binary classification, otherwise keep dimensions from model output\n",
    "            test_loss += loss_fn(pred, y.type(torch.FloatTensor).to(device)).item()\n",
    "            correct += (pred.round() == y).type(torch.float).sum().item()\n",
    "            \n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    if test_set:\n",
    "        print(f\"Test set Error: \\n Test Set Accuracy: {(100*correct):>0.5f}%, Avg Test Set loss: {test_loss:>8f} \\n\")\n",
    "    else:\n",
    "        print(f\"Training Set Error: \\n Training Set Accuracy: {(100*correct):>0.5f}%, Avg Training Set loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test data (Commented out: Changed to loading different dataset class instances)\n",
    "#trainSamples = int(ds1.__len__()*0.7)\n",
    "#testSamples = int(ds1.__len__() - trainSamples)\n",
    "#training_data, test_data = random_split(ds1, (trainSamples,testSamples), generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "#train_dataloader = DataLoader(training_data, batch_size=64, drop_last = True) # Drop_last, to avoid incomplete batches, which won't work with weighted loss\n",
    "#test_dataloader = DataLoader(test_data, batch_size=64, drop_last = True) # Drop_last, to avoid incomplete batches, which won't work with weighted loss\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(ds1, batch_size=64, drop_last = True) # Drop_last, to avoid incomplete batches, which won't work with weighted loss\n",
    "test_dataloader = DataLoader(ds2, batch_size=64, drop_last = True) # Drop_last, to avoid incomplete batches, which won't work with weighted loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.519735  [    0/6508495]\n",
      "loss: 0.512215  [64000/6508495]\n",
      "loss: 0.556209  [128000/6508495]\n",
      "loss: 0.527053  [192000/6508495]\n",
      "loss: 0.512514  [256000/6508495]\n",
      "loss: 0.510095  [320000/6508495]\n",
      "loss: 0.521261  [384000/6508495]\n",
      "loss: 0.507959  [448000/6508495]\n",
      "loss: 0.503416  [512000/6508495]\n",
      "loss: 0.534994  [576000/6508495]\n",
      "loss: 0.542025  [640000/6508495]\n",
      "loss: 0.519001  [704000/6508495]\n",
      "loss: 0.522949  [768000/6508495]\n",
      "loss: 0.525378  [832000/6508495]\n",
      "loss: 0.509336  [896000/6508495]\n",
      "loss: 0.522680  [960000/6508495]\n",
      "loss: 0.500942  [1024000/6508495]\n",
      "loss: 0.513819  [1088000/6508495]\n",
      "loss: 0.528224  [1152000/6508495]\n",
      "loss: 0.507429  [1216000/6508495]\n",
      "loss: 0.500746  [1280000/6508495]\n",
      "loss: 0.500791  [1344000/6508495]\n",
      "loss: 0.506477  [1408000/6508495]\n",
      "loss: 0.520566  [1472000/6508495]\n",
      "loss: 0.509483  [1536000/6508495]\n",
      "loss: 0.509382  [1600000/6508495]\n",
      "loss: 0.518198  [1664000/6508495]\n",
      "loss: 0.746179  [1728000/6508495]\n",
      "loss: 0.584372  [1792000/6508495]\n",
      "loss: 0.523499  [1856000/6508495]\n",
      "loss: 0.508152  [1920000/6508495]\n",
      "loss: 0.508986  [1984000/6508495]\n",
      "loss: 0.509593  [2048000/6508495]\n",
      "loss: 0.505732  [2112000/6508495]\n",
      "loss: 0.550876  [2176000/6508495]\n",
      "loss: 0.507250  [2240000/6508495]\n",
      "loss: 0.519688  [2304000/6508495]\n",
      "loss: 0.507132  [2368000/6508495]\n",
      "loss: 0.508078  [2432000/6508495]\n",
      "loss: 0.515226  [2496000/6508495]\n",
      "loss: 0.515696  [2560000/6508495]\n",
      "loss: 0.516060  [2624000/6508495]\n",
      "loss: 0.507436  [2688000/6508495]\n",
      "loss: 0.503041  [2752000/6508495]\n",
      "loss: 0.524592  [2816000/6508495]\n",
      "loss: 0.571814  [2880000/6508495]\n",
      "loss: 0.502012  [2944000/6508495]\n",
      "loss: 0.508234  [3008000/6508495]\n",
      "loss: 0.511935  [3072000/6508495]\n",
      "loss: 0.508435  [3136000/6508495]\n",
      "loss: 0.519898  [3200000/6508495]\n",
      "loss: 0.517843  [3264000/6508495]\n",
      "loss: 0.524940  [3328000/6508495]\n",
      "loss: 0.513941  [3392000/6508495]\n",
      "loss: 0.502232  [3456000/6508495]\n",
      "loss: 0.504570  [3520000/6508495]\n",
      "loss: 0.517892  [3584000/6508495]\n",
      "loss: 0.501087  [3648000/6508495]\n",
      "loss: 0.511520  [3712000/6508495]\n",
      "loss: 0.506332  [3776000/6508495]\n",
      "loss: 0.506543  [3840000/6508495]\n",
      "loss: 0.531572  [3904000/6508495]\n",
      "loss: 0.509709  [3968000/6508495]\n",
      "loss: 0.501938  [4032000/6508495]\n",
      "loss: 0.500734  [4096000/6508495]\n",
      "loss: 0.506973  [4160000/6508495]\n",
      "loss: 0.506568  [4224000/6508495]\n",
      "loss: 0.511519  [4288000/6508495]\n",
      "loss: 0.508381  [4352000/6508495]\n",
      "loss: 0.688558  [4416000/6508495]\n",
      "loss: 0.513810  [4480000/6508495]\n",
      "loss: 0.522488  [4544000/6508495]\n",
      "loss: 0.512757  [4608000/6508495]\n",
      "loss: 0.508166  [4672000/6508495]\n",
      "loss: 0.500893  [4736000/6508495]\n",
      "loss: 0.514314  [4800000/6508495]\n",
      "loss: 0.500717  [4864000/6508495]\n",
      "loss: 0.511993  [4928000/6508495]\n",
      "loss: 0.507604  [4992000/6508495]\n",
      "loss: 0.532864  [5056000/6508495]\n",
      "loss: 0.515781  [5120000/6508495]\n",
      "loss: 0.509262  [5184000/6508495]\n",
      "loss: 0.514476  [5248000/6508495]\n",
      "loss: 0.511994  [5312000/6508495]\n",
      "loss: 0.500889  [5376000/6508495]\n",
      "loss: 0.512771  [5440000/6508495]\n",
      "loss: 0.500694  [5504000/6508495]\n",
      "loss: 0.525051  [5568000/6508495]\n",
      "loss: 0.633459  [5632000/6508495]\n",
      "loss: 0.544808  [5696000/6508495]\n",
      "loss: 0.505682  [5760000/6508495]\n",
      "loss: 0.512360  [5824000/6508495]\n",
      "loss: 0.592179  [5888000/6508495]\n",
      "loss: 0.511369  [5952000/6508495]\n",
      "loss: 0.519193  [6016000/6508495]\n",
      "loss: 0.509539  [6080000/6508495]\n",
      "loss: 0.507881  [6144000/6508495]\n",
      "loss: 0.525951  [6208000/6508495]\n",
      "loss: 0.516409  [6272000/6508495]\n",
      "loss: 0.527542  [6336000/6508495]\n",
      "loss: 0.532119  [6400000/6508495]\n",
      "loss: 0.500938  [6464000/6508495]\n",
      "test loop!!!!!!!!!!!\n",
      "Test set Error: \n",
      " Test Set Accuracy: 87.88116%, Avg Test Set loss: 0.554956 \n",
      "\n",
      "test loop!!!!!!!!!!!\n",
      "Training Set Error: \n",
      " Training Set Accuracy: 94.02728%, Avg Training Set loss: 0.527639 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.518482  [    0/6508495]\n",
      "loss: 0.500825  [64000/6508495]\n",
      "loss: 0.574706  [128000/6508495]\n",
      "loss: 0.507621  [192000/6508495]\n",
      "loss: 0.506614  [256000/6508495]\n",
      "loss: 0.517931  [320000/6508495]\n",
      "loss: 0.518443  [384000/6508495]\n",
      "loss: 0.500960  [448000/6508495]\n",
      "loss: 0.521050  [512000/6508495]\n",
      "loss: 0.558857  [576000/6508495]\n",
      "loss: 0.527750  [640000/6508495]\n",
      "loss: 0.512257  [704000/6508495]\n",
      "loss: 0.517283  [768000/6508495]\n",
      "loss: 0.518897  [832000/6508495]\n",
      "loss: 0.503627  [896000/6508495]\n",
      "loss: 0.518895  [960000/6508495]\n",
      "loss: 0.523102  [1024000/6508495]\n",
      "loss: 0.524590  [1088000/6508495]\n",
      "loss: 0.514408  [1152000/6508495]\n",
      "loss: 0.538203  [1216000/6508495]\n",
      "loss: 0.506561  [1280000/6508495]\n",
      "loss: 0.505168  [1344000/6508495]\n",
      "loss: 0.531871  [1408000/6508495]\n",
      "loss: 0.518916  [1472000/6508495]\n",
      "loss: 0.506480  [1536000/6508495]\n",
      "loss: 0.506399  [1600000/6508495]\n",
      "loss: 0.519108  [1664000/6508495]\n",
      "loss: 0.536449  [1728000/6508495]\n",
      "loss: 0.708581  [1792000/6508495]\n",
      "loss: 0.506692  [1856000/6508495]\n",
      "loss: 0.514920  [1920000/6508495]\n",
      "loss: 0.513130  [1984000/6508495]\n",
      "loss: 0.509808  [2048000/6508495]\n",
      "loss: 0.500792  [2112000/6508495]\n",
      "loss: 0.532926  [2176000/6508495]\n",
      "loss: 0.517112  [2240000/6508495]\n",
      "loss: 0.535808  [2304000/6508495]\n",
      "loss: 0.506730  [2368000/6508495]\n",
      "loss: 0.515999  [2432000/6508495]\n",
      "loss: 0.515805  [2496000/6508495]\n",
      "loss: 0.502968  [2560000/6508495]\n",
      "loss: 0.525750  [2624000/6508495]\n",
      "loss: 0.528604  [2688000/6508495]\n",
      "loss: 0.525221  [2752000/6508495]\n",
      "loss: 0.518041  [2816000/6508495]\n",
      "loss: 0.543310  [2880000/6508495]\n",
      "loss: 0.501213  [2944000/6508495]\n",
      "loss: 0.512107  [3008000/6508495]\n",
      "loss: 0.520776  [3072000/6508495]\n",
      "loss: 0.513771  [3136000/6508495]\n",
      "loss: 0.518104  [3200000/6508495]\n",
      "loss: 0.507922  [3264000/6508495]\n",
      "loss: 0.506965  [3328000/6508495]\n",
      "loss: 0.509382  [3392000/6508495]\n",
      "loss: 0.509057  [3456000/6508495]\n",
      "loss: 0.506166  [3520000/6508495]\n",
      "loss: 0.516245  [3584000/6508495]\n",
      "loss: 0.502145  [3648000/6508495]\n",
      "loss: 0.513930  [3712000/6508495]\n",
      "loss: 0.505648  [3776000/6508495]\n",
      "loss: 0.507721  [3840000/6508495]\n",
      "loss: 0.531755  [3904000/6508495]\n",
      "loss: 0.502230  [3968000/6508495]\n",
      "loss: 0.501652  [4032000/6508495]\n",
      "loss: 0.503733  [4096000/6508495]\n",
      "loss: 0.506229  [4160000/6508495]\n",
      "loss: 0.509135  [4224000/6508495]\n",
      "loss: 0.523675  [4288000/6508495]\n",
      "loss: 0.678037  [4352000/6508495]\n",
      "loss: 0.506608  [4416000/6508495]\n",
      "loss: 0.507767  [4480000/6508495]\n",
      "loss: 0.536170  [4544000/6508495]\n",
      "loss: 0.501248  [4608000/6508495]\n",
      "loss: 0.502406  [4672000/6508495]\n",
      "loss: 0.505033  [4736000/6508495]\n",
      "loss: 0.507084  [4800000/6508495]\n",
      "loss: 0.512156  [4864000/6508495]\n",
      "loss: 0.500705  [4928000/6508495]\n",
      "loss: 0.500701  [4992000/6508495]\n",
      "loss: 0.530436  [5056000/6508495]\n",
      "loss: 0.507084  [5120000/6508495]\n",
      "loss: 0.507540  [5184000/6508495]\n",
      "loss: 0.507854  [5248000/6508495]\n",
      "loss: 0.510555  [5312000/6508495]\n",
      "loss: 0.500812  [5376000/6508495]\n",
      "loss: 0.500863  [5440000/6508495]\n",
      "loss: 0.515973  [5504000/6508495]\n",
      "loss: 0.523952  [5568000/6508495]\n",
      "loss: 0.652473  [5632000/6508495]\n",
      "loss: 0.538462  [5696000/6508495]\n",
      "loss: 0.500871  [5760000/6508495]\n",
      "loss: 0.508147  [5824000/6508495]\n",
      "loss: 0.571916  [5888000/6508495]\n",
      "loss: 0.516184  [5952000/6508495]\n",
      "loss: 0.504609  [6016000/6508495]\n",
      "loss: 0.515022  [6080000/6508495]\n",
      "loss: 0.701781  [6144000/6508495]\n",
      "loss: 0.558053  [6208000/6508495]\n",
      "loss: 0.515719  [6272000/6508495]\n",
      "loss: 0.522791  [6336000/6508495]\n",
      "loss: 0.533057  [6400000/6508495]\n",
      "loss: 0.506168  [6464000/6508495]\n",
      "test loop!!!!!!!!!!!\n",
      "Test set Error: \n",
      " Test Set Accuracy: 88.45212%, Avg Test Set loss: 0.550742 \n",
      "\n",
      "test loop!!!!!!!!!!!\n",
      "Training Set Error: \n",
      " Training Set Accuracy: 93.98810%, Avg Training Set loss: 0.526562 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.508032  [    0/6508495]\n",
      "loss: 0.502300  [64000/6508495]\n",
      "loss: 0.560987  [128000/6508495]\n",
      "loss: 0.520347  [192000/6508495]\n",
      "loss: 0.502468  [256000/6508495]\n",
      "loss: 0.507556  [320000/6508495]\n",
      "loss: 0.508276  [384000/6508495]\n",
      "loss: 0.518051  [448000/6508495]\n",
      "loss: 0.513635  [512000/6508495]\n",
      "loss: 0.800057  [576000/6508495]\n",
      "loss: 0.580455  [640000/6508495]\n",
      "loss: 0.515040  [704000/6508495]\n",
      "loss: 0.514979  [768000/6508495]\n",
      "loss: 0.539322  [832000/6508495]\n",
      "loss: 0.506836  [896000/6508495]\n",
      "loss: 0.523870  [960000/6508495]\n",
      "loss: 0.506655  [1024000/6508495]\n",
      "loss: 0.520528  [1088000/6508495]\n",
      "loss: 0.528346  [1152000/6508495]\n",
      "loss: 0.518664  [1216000/6508495]\n",
      "loss: 0.511200  [1280000/6508495]\n",
      "loss: 0.501979  [1344000/6508495]\n",
      "loss: 0.518417  [1408000/6508495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.512713  [1472000/6508495]\n",
      "loss: 0.525720  [1536000/6508495]\n",
      "loss: 0.506764  [1600000/6508495]\n",
      "loss: 0.501105  [1664000/6508495]\n",
      "loss: 0.698442  [1728000/6508495]\n",
      "loss: 0.654067  [1792000/6508495]\n",
      "loss: 0.500708  [1856000/6508495]\n",
      "loss: 0.512716  [1920000/6508495]\n",
      "loss: 0.518449  [1984000/6508495]\n",
      "loss: 0.524499  [2048000/6508495]\n",
      "loss: 0.501677  [2112000/6508495]\n",
      "loss: 0.535989  [2176000/6508495]\n",
      "loss: 0.519756  [2240000/6508495]\n",
      "loss: 0.521621  [2304000/6508495]\n",
      "loss: 0.505773  [2368000/6508495]\n",
      "loss: 0.516232  [2432000/6508495]\n",
      "loss: 0.516848  [2496000/6508495]\n",
      "loss: 0.506419  [2560000/6508495]\n",
      "loss: 0.529178  [2624000/6508495]\n",
      "loss: 0.510910  [2688000/6508495]\n",
      "loss: 0.531904  [2752000/6508495]\n",
      "loss: 0.513999  [2816000/6508495]\n",
      "loss: 0.534075  [2880000/6508495]\n",
      "loss: 0.500729  [2944000/6508495]\n",
      "loss: 0.516385  [3008000/6508495]\n",
      "loss: 0.510174  [3072000/6508495]\n",
      "loss: 0.512477  [3136000/6508495]\n",
      "loss: 0.502875  [3200000/6508495]\n",
      "loss: 0.521026  [3264000/6508495]\n",
      "loss: 0.509347  [3328000/6508495]\n",
      "loss: 0.507083  [3392000/6508495]\n",
      "loss: 0.506578  [3456000/6508495]\n",
      "loss: 0.513106  [3520000/6508495]\n",
      "loss: 0.517975  [3584000/6508495]\n",
      "loss: 0.500709  [3648000/6508495]\n",
      "loss: 0.522856  [3712000/6508495]\n",
      "loss: 0.518131  [3776000/6508495]\n",
      "loss: 0.500694  [3840000/6508495]\n",
      "loss: 0.520256  [3904000/6508495]\n",
      "loss: 0.502324  [3968000/6508495]\n",
      "loss: 0.500790  [4032000/6508495]\n",
      "loss: 0.512375  [4096000/6508495]\n",
      "loss: 0.501459  [4160000/6508495]\n",
      "loss: 0.503126  [4224000/6508495]\n",
      "loss: 0.521047  [4288000/6508495]\n",
      "loss: 0.536987  [4352000/6508495]\n",
      "loss: 0.508768  [4416000/6508495]\n",
      "loss: 0.519316  [4480000/6508495]\n",
      "loss: 0.528046  [4544000/6508495]\n",
      "loss: 0.501828  [4608000/6508495]\n",
      "loss: 0.513133  [4672000/6508495]\n",
      "loss: 0.500697  [4736000/6508495]\n",
      "loss: 0.512329  [4800000/6508495]\n",
      "loss: 0.512373  [4864000/6508495]\n",
      "loss: 0.519103  [4928000/6508495]\n",
      "loss: 0.500695  [4992000/6508495]\n",
      "loss: 0.565959  [5056000/6508495]\n",
      "loss: 0.505266  [5120000/6508495]\n",
      "loss: 0.501037  [5184000/6508495]\n",
      "loss: 0.521037  [5248000/6508495]\n",
      "loss: 0.532131  [5312000/6508495]\n",
      "loss: 0.505489  [5376000/6508495]\n",
      "loss: 0.512491  [5440000/6508495]\n",
      "loss: 0.501778  [5504000/6508495]\n",
      "loss: 0.506935  [5568000/6508495]\n",
      "loss: 0.653794  [5632000/6508495]\n",
      "loss: 0.549120  [5696000/6508495]\n",
      "loss: 0.512846  [5760000/6508495]\n",
      "loss: 0.508694  [5824000/6508495]\n",
      "loss: 0.576396  [5888000/6508495]\n",
      "loss: 0.516961  [5952000/6508495]\n",
      "loss: 0.533966  [6016000/6508495]\n",
      "loss: 0.507856  [6080000/6508495]\n",
      "loss: 0.500868  [6144000/6508495]\n",
      "loss: 0.546066  [6208000/6508495]\n",
      "loss: 0.530125  [6272000/6508495]\n",
      "loss: 0.526910  [6336000/6508495]\n",
      "loss: 0.518842  [6400000/6508495]\n",
      "loss: 0.505952  [6464000/6508495]\n",
      "test loop!!!!!!!!!!!\n",
      "Test set Error: \n",
      " Test Set Accuracy: 88.24639%, Avg Test Set loss: 0.553011 \n",
      "\n",
      "test loop!!!!!!!!!!!\n",
      "Training Set Error: \n",
      " Training Set Accuracy: 94.28421%, Avg Training Set loss: 0.526383 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.515378  [    0/6508495]\n",
      "loss: 0.512306  [64000/6508495]\n",
      "loss: 0.594263  [128000/6508495]\n",
      "loss: 0.534982  [192000/6508495]\n",
      "loss: 0.511745  [256000/6508495]\n",
      "loss: 0.513453  [320000/6508495]\n",
      "loss: 0.542631  [384000/6508495]\n",
      "loss: 0.512475  [448000/6508495]\n",
      "loss: 0.522064  [512000/6508495]\n",
      "loss: 0.512162  [576000/6508495]\n",
      "loss: 0.511697  [640000/6508495]\n",
      "loss: 0.501762  [704000/6508495]\n",
      "loss: 0.514615  [768000/6508495]\n",
      "loss: 0.528826  [832000/6508495]\n",
      "loss: 0.517994  [896000/6508495]\n",
      "loss: 0.517483  [960000/6508495]\n",
      "loss: 0.500923  [1024000/6508495]\n",
      "loss: 0.514097  [1088000/6508495]\n",
      "loss: 0.512665  [1152000/6508495]\n",
      "loss: 0.507065  [1216000/6508495]\n",
      "loss: 0.500694  [1280000/6508495]\n",
      "loss: 0.503622  [1344000/6508495]\n",
      "loss: 0.504878  [1408000/6508495]\n",
      "loss: 0.518288  [1472000/6508495]\n",
      "loss: 0.510824  [1536000/6508495]\n",
      "loss: 0.500694  [1600000/6508495]\n",
      "loss: 0.513410  [1664000/6508495]\n",
      "loss: 0.544000  [1728000/6508495]\n",
      "loss: 0.794598  [1792000/6508495]\n",
      "loss: 0.506536  [1856000/6508495]\n",
      "loss: 0.513549  [1920000/6508495]\n",
      "loss: 0.519541  [1984000/6508495]\n",
      "loss: 0.500758  [2048000/6508495]\n",
      "loss: 0.504375  [2112000/6508495]\n",
      "loss: 0.535645  [2176000/6508495]\n",
      "loss: 0.506447  [2240000/6508495]\n",
      "loss: 0.508099  [2304000/6508495]\n",
      "loss: 0.500994  [2368000/6508495]\n",
      "loss: 0.517832  [2432000/6508495]\n",
      "loss: 0.531709  [2496000/6508495]\n",
      "loss: 0.508395  [2560000/6508495]\n",
      "loss: 0.522509  [2624000/6508495]\n",
      "loss: 0.517094  [2688000/6508495]\n",
      "loss: 0.546138  [2752000/6508495]\n",
      "loss: 0.507358  [2816000/6508495]\n",
      "loss: 0.530619  [2880000/6508495]\n",
      "loss: 0.503371  [2944000/6508495]\n",
      "loss: 0.505810  [3008000/6508495]\n",
      "loss: 0.511478  [3072000/6508495]\n",
      "loss: 0.512979  [3136000/6508495]\n",
      "loss: 0.519272  [3200000/6508495]\n",
      "loss: 0.515471  [3264000/6508495]\n",
      "loss: 0.506489  [3328000/6508495]\n",
      "loss: 0.506835  [3392000/6508495]\n",
      "loss: 0.500694  [3456000/6508495]\n",
      "loss: 0.508932  [3520000/6508495]\n",
      "loss: 0.529391  [3584000/6508495]\n",
      "loss: 0.500804  [3648000/6508495]\n",
      "loss: 0.516952  [3712000/6508495]\n",
      "loss: 0.502331  [3776000/6508495]\n",
      "loss: 0.500701  [3840000/6508495]\n",
      "loss: 0.531217  [3904000/6508495]\n",
      "loss: 0.509549  [3968000/6508495]\n",
      "loss: 0.506822  [4032000/6508495]\n",
      "loss: 0.506564  [4096000/6508495]\n",
      "loss: 0.504809  [4160000/6508495]\n",
      "loss: 0.523103  [4224000/6508495]\n",
      "loss: 0.520649  [4288000/6508495]\n",
      "loss: 0.519046  [4352000/6508495]\n",
      "loss: 0.521635  [4416000/6508495]\n",
      "loss: 0.525580  [4480000/6508495]\n",
      "loss: 0.523093  [4544000/6508495]\n",
      "loss: 0.506571  [4608000/6508495]\n",
      "loss: 0.506508  [4672000/6508495]\n",
      "loss: 0.500704  [4736000/6508495]\n",
      "loss: 0.506536  [4800000/6508495]\n",
      "loss: 0.504258  [4864000/6508495]\n",
      "loss: 0.507974  [4928000/6508495]\n",
      "loss: 0.501092  [4992000/6508495]\n",
      "loss: 0.586454  [5056000/6508495]\n",
      "loss: 0.513727  [5120000/6508495]\n",
      "loss: 0.501266  [5184000/6508495]\n",
      "loss: 0.526619  [5248000/6508495]\n",
      "loss: 0.512803  [5312000/6508495]\n",
      "loss: 0.506545  [5376000/6508495]\n",
      "loss: 0.501888  [5440000/6508495]\n",
      "loss: 0.500694  [5504000/6508495]\n",
      "loss: 0.522864  [5568000/6508495]\n",
      "loss: 0.631326  [5632000/6508495]\n",
      "loss: 0.535243  [5696000/6508495]\n",
      "loss: 0.513649  [5760000/6508495]\n",
      "loss: 0.512376  [5824000/6508495]\n",
      "loss: 0.586711  [5888000/6508495]\n",
      "loss: 0.522014  [5952000/6508495]\n",
      "loss: 0.506515  [6016000/6508495]\n",
      "loss: 0.519435  [6080000/6508495]\n",
      "loss: 0.503817  [6144000/6508495]\n",
      "loss: 0.531954  [6208000/6508495]\n",
      "loss: 0.508271  [6272000/6508495]\n",
      "loss: 0.516528  [6336000/6508495]\n",
      "loss: 0.546760  [6400000/6508495]\n",
      "loss: 0.500739  [6464000/6508495]\n",
      "test loop!!!!!!!!!!!\n",
      "Test set Error: \n",
      " Test Set Accuracy: 88.38229%, Avg Test Set loss: 0.552002 \n",
      "\n",
      "test loop!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    \n",
    "    train_loop(train_dataloader, model, loss, optimizer)    \n",
    "    test_loop(test_dataloader, model, loss)\n",
    "    test_loop(train_dataloader, model, loss, test_set = False)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d334debdbe0c15d06a22e1a5d8333410545070b029a2d558077d9bd2658c7aed"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
