{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import EEGDataset\n",
    "from torch.utils.data import random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set\n",
      "../data/study_1A_mat_simple/S_01/night_1/artefact_annotations.npy\n",
      "../data/study_1A_mat_simple/S_01/night_1/EEG_raw_250hz.npy\n",
      "../data/study_1A_mat_simple/S_01/night_2/artefact_annotations.npy\n",
      "../data/study_1A_mat_simple/S_01/night_2/EEG_raw_250hz.npy\n",
      "../data/study_1A_mat_simple/S_01/night_3/artefact_annotations.npy\n",
      "../data/study_1A_mat_simple/S_01/night_3/EEG_raw_250hz.npy\n",
      "../data/study_1A_mat_simple/S_01/night_4/artefact_annotations.npy\n",
      "../data/study_1A_mat_simple/S_01/night_4/EEG_raw_250hz.npy\n",
      "../data/study_1A_mat_simple/S_02/night_1/artefact_annotations.npy\n",
      "../data/study_1A_mat_simple/S_02/night_1/EEG_raw_250hz.npy\n",
      "../data/study_1A_mat_simple/S_02/night_2/artefact_annotations.npy\n",
      "../data/study_1A_mat_simple/S_02/night_2/EEG_raw_250hz.npy\n",
      "../data/study_1A_mat_simple/S_02/night_3/artefact_annotations.npy\n",
      "../data/study_1A_mat_simple/S_02/night_3/EEG_raw_250hz.npy\n",
      "../data/study_1A_mat_simple/S_02/night_4/artefact_annotations.npy\n",
      "../data/study_1A_mat_simple/S_02/night_4/EEG_raw_250hz.npy\n",
      "../data/study_1A_mat_simple/S_03/night_1/artefact_annotations.npy\n",
      "../data/study_1A_mat_simple/S_03/night_1/EEG_raw_250hz.npy\n",
      "../data/study_1A_mat_simple/S_03/night_2/artefact_annotations.npy\n",
      "Lables for night 0 loaded\n",
      "../data/study_1A_mat_simple/S_03/night_2/EEG_raw_250hz.npy\n",
      "Night 0 data loaded\n",
      "../data/study_1A_mat_simple/S_03/night_3/artefact_annotations.npy\n",
      "Lables for night 1 loaded\n",
      "../data/study_1A_mat_simple/S_03/night_3/EEG_raw_250hz.npy\n",
      "Night 1 data loaded\n",
      "Training set\n",
      "../data/study_1A_mat_simple/S_01/night_1/artefact_annotations.npy\n",
      "Lables for night 0 loaded\n",
      "../data/study_1A_mat_simple/S_01/night_1/EEG_raw_250hz.npy\n",
      "Night 0 data loaded\n",
      "../data/study_1A_mat_simple/S_01/night_2/artefact_annotations.npy\n",
      "Lables for night 1 loaded\n",
      "../data/study_1A_mat_simple/S_01/night_2/EEG_raw_250hz.npy\n",
      "Night 1 data loaded\n",
      "../data/study_1A_mat_simple/S_01/night_3/artefact_annotations.npy\n",
      "Lables for night 2 loaded\n",
      "../data/study_1A_mat_simple/S_01/night_3/EEG_raw_250hz.npy\n",
      "Night 2 data loaded\n",
      "../data/study_1A_mat_simple/S_01/night_4/artefact_annotations.npy\n",
      "Lables for night 3 loaded\n",
      "../data/study_1A_mat_simple/S_01/night_4/EEG_raw_250hz.npy\n",
      "Night 3 data loaded\n",
      "../data/study_1A_mat_simple/S_02/night_1/artefact_annotations.npy\n",
      "Lables for night 4 loaded\n",
      "../data/study_1A_mat_simple/S_02/night_1/EEG_raw_250hz.npy\n",
      "Night 4 data loaded\n",
      "../data/study_1A_mat_simple/S_02/night_2/artefact_annotations.npy\n",
      "Lables for night 5 loaded\n",
      "../data/study_1A_mat_simple/S_02/night_2/EEG_raw_250hz.npy\n",
      "Night 5 data loaded\n",
      "../data/study_1A_mat_simple/S_02/night_3/artefact_annotations.npy\n",
      "Lables for night 6 loaded\n",
      "../data/study_1A_mat_simple/S_02/night_3/EEG_raw_250hz.npy\n",
      "Night 6 data loaded\n",
      "../data/study_1A_mat_simple/S_02/night_4/artefact_annotations.npy\n",
      "Lables for night 7 loaded\n",
      "../data/study_1A_mat_simple/S_02/night_4/EEG_raw_250hz.npy\n",
      "Night 7 data loaded\n",
      "../data/study_1A_mat_simple/S_03/night_1/artefact_annotations.npy\n",
      "Lables for night 8 loaded\n",
      "../data/study_1A_mat_simple/S_03/night_1/EEG_raw_250hz.npy\n",
      "Night 8 data loaded\n"
     ]
    }
   ],
   "source": [
    "# load in the dataset\n",
    "\n",
    "#raw_data_dir = '//uni.au.dk/dfs/Tech_EarEEG/Students/RD2022_Artefact_AkselStark/data/1A/study_1A_mat_simple'\n",
    "raw_data_dir = '../data'\n",
    "\n",
    "trainingNights = 9\n",
    "testNights = 2\n",
    "\n",
    "print(\"Test set\")\n",
    "ds2 = EEGDataset(raw_data_dir,testNights, 250, skips = trainingNights)\n",
    "\n",
    "print(\"Training set\")\n",
    "ds1 = EEGDataset(raw_data_dir,trainingNights, 250, skips = 0) #Instantiate a dataset using the directory of data, amount of night to include and amount of samples in a segment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug\n"
     ]
    }
   ],
   "source": [
    "#Calculate class imbalance\n",
    "balancing_dataloader = DataLoader(ds1, drop_last = True) # Drop_last, to avoid incomplete batches, which won't work with weighted loss\n",
    "artefacts = 0\n",
    "good_samples = 0\n",
    "for batch, (X, y) in enumerate(balancing_dataloader):\n",
    "    if y == 1:\n",
    "        artefacts += 1\n",
    "    else:\n",
    "        good_samples += 1\n",
    "    if batch > 100000:\n",
    "        break\n",
    "\n",
    "class_ratio = good_samples/artefacts\n",
    "print(\"debug\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' #Check for cuda \n",
    "print(f'Using {device} device')\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        #self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.BatchNorm1d(250),\n",
    "            nn.Linear(250, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid(), \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model.to(device)\n",
    "\n",
    "#pos_weight: amount of positive examples compared to negative examples. Calculate as: negative_examples/positive_examples\n",
    "loss = nn.BCEWithLogitsLoss(pos_weight = class_ratio*torch.ones([batch_size]).to(device)) \n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)# Reshape to 1 dimension if using binary classification, otherwise keep dimensions from model output\n",
    "        pred = pred.reshape(-1)\n",
    "        pred = pred.to(device)\n",
    "        yFloat = y.type(torch.FloatTensor).to(device)\n",
    "        \n",
    "        loss = loss_fn(pred, yFloat)\n",
    "        \n",
    "\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            #print(\"Predicted values:\")\n",
    "            #print(pred)\n",
    "            #print(\"Actual values:\")\n",
    "            #print(yFloat)\n",
    "\n",
    "\n",
    "\n",
    "def test_loop(dataloader_test, model, loss_fn, test_set = True):\n",
    "    size = len(dataloader_test.dataset)\n",
    "    num_batches = len(dataloader_test)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader_test:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X).reshape(-1).to(device) # Reshape to 1 dimension if using binary classification, otherwise keep dimensions from model output\n",
    "            test_loss += loss_fn(pred, y.type(torch.FloatTensor).to(device)).item()\n",
    "            correct += (pred.round() == y).type(torch.float).sum().item()\n",
    "            \n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    if test_set:\n",
    "        print(f\"Test set Error: \\n Test Set Accuracy: {(100*correct):>0.5f}%, Avg Test Set loss: {test_loss:>8f} \\n\")\n",
    "    else:\n",
    "        print(f\"Training Set Error: \\n Training Set Accuracy: {(100*correct):>0.5f}%, Avg Training Set loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test data (Commented out: Changed to loading different dataset class instances)\n",
    "#trainSamples = int(ds1.__len__()*0.7)\n",
    "#testSamples = int(ds1.__len__() - trainSamples)\n",
    "#training_data, test_data = random_split(ds1, (trainSamples,testSamples), generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "#train_dataloader = DataLoader(training_data, batch_size=64, drop_last = True) # Drop_last, to avoid incomplete batches, which won't work with weighted loss\n",
    "#test_dataloader = DataLoader(test_data, batch_size=64, drop_last = True) # Drop_last, to avoid incomplete batches, which won't work with weighted loss\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(ds1, batch_size=64, drop_last = True) # Drop_last, to avoid incomplete batches, which won't work with weighted loss\n",
    "test_dataloader = DataLoader(ds2, batch_size=64, drop_last = True) # Drop_last, to avoid incomplete batches, which won't work with weighted loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.719505  [    0/6508495]\n",
      "loss: 0.708488  [64000/6508495]\n",
      "loss: 0.712196  [128000/6508495]\n",
      "loss: 0.680704  [192000/6508495]\n",
      "loss: 0.656429  [256000/6508495]\n",
      "loss: 0.628320  [320000/6508495]\n",
      "loss: 0.595436  [384000/6508495]\n",
      "loss: 0.578245  [448000/6508495]\n",
      "loss: 0.572842  [512000/6508495]\n",
      "loss: 0.617983  [576000/6508495]\n",
      "loss: 0.583535  [640000/6508495]\n",
      "loss: 0.584873  [704000/6508495]\n",
      "loss: 0.585472  [768000/6508495]\n",
      "loss: 0.608247  [832000/6508495]\n",
      "loss: 0.572189  [896000/6508495]\n",
      "loss: 0.579306  [960000/6508495]\n",
      "loss: 0.586438  [1024000/6508495]\n",
      "loss: 0.590747  [1088000/6508495]\n",
      "loss: 0.584993  [1152000/6508495]\n",
      "loss: 0.563189  [1216000/6508495]\n",
      "loss: 0.581698  [1280000/6508495]\n",
      "loss: 0.587315  [1344000/6508495]\n",
      "loss: 0.579288  [1408000/6508495]\n",
      "loss: 0.582122  [1472000/6508495]\n",
      "loss: 0.582383  [1536000/6508495]\n",
      "loss: 0.572791  [1600000/6508495]\n",
      "loss: 0.594168  [1664000/6508495]\n",
      "loss: 0.637254  [1728000/6508495]\n",
      "loss: 0.633589  [1792000/6508495]\n",
      "loss: 0.575163  [1856000/6508495]\n",
      "loss: 0.579629  [1920000/6508495]\n",
      "loss: 0.586930  [1984000/6508495]\n",
      "loss: 0.559521  [2048000/6508495]\n",
      "loss: 0.576547  [2112000/6508495]\n",
      "loss: 0.603943  [2176000/6508495]\n",
      "loss: 0.575228  [2240000/6508495]\n",
      "loss: 0.573756  [2304000/6508495]\n",
      "loss: 0.585498  [2368000/6508495]\n",
      "loss: 0.568507  [2432000/6508495]\n",
      "loss: 0.561032  [2496000/6508495]\n",
      "loss: 0.554353  [2560000/6508495]\n",
      "loss: 0.574970  [2624000/6508495]\n",
      "loss: 0.581397  [2688000/6508495]\n",
      "loss: 0.583857  [2752000/6508495]\n",
      "loss: 0.569240  [2816000/6508495]\n",
      "loss: 0.557033  [2880000/6508495]\n",
      "loss: 0.577727  [2944000/6508495]\n",
      "loss: 0.578690  [3008000/6508495]\n",
      "loss: 0.584874  [3072000/6508495]\n",
      "loss: 0.542055  [3136000/6508495]\n",
      "loss: 0.570381  [3200000/6508495]\n",
      "loss: 0.562741  [3264000/6508495]\n",
      "loss: 0.555986  [3328000/6508495]\n",
      "loss: 0.596891  [3392000/6508495]\n",
      "loss: 0.565150  [3456000/6508495]\n",
      "loss: 0.573460  [3520000/6508495]\n",
      "loss: 0.582352  [3584000/6508495]\n",
      "loss: 0.562976  [3648000/6508495]\n",
      "loss: 0.565940  [3712000/6508495]\n",
      "loss: 0.561923  [3776000/6508495]\n",
      "loss: 0.557335  [3840000/6508495]\n",
      "loss: 0.569573  [3904000/6508495]\n",
      "loss: 0.553793  [3968000/6508495]\n",
      "loss: 0.564606  [4032000/6508495]\n",
      "loss: 0.569661  [4096000/6508495]\n",
      "loss: 0.533210  [4160000/6508495]\n",
      "loss: 0.540824  [4224000/6508495]\n",
      "loss: 0.546989  [4288000/6508495]\n",
      "loss: 0.549222  [4352000/6508495]\n",
      "loss: 0.568767  [4416000/6508495]\n",
      "loss: 0.568967  [4480000/6508495]\n",
      "loss: 0.577421  [4544000/6508495]\n",
      "loss: 0.557773  [4608000/6508495]\n",
      "loss: 0.536355  [4672000/6508495]\n",
      "loss: 0.532143  [4736000/6508495]\n",
      "loss: 0.533951  [4800000/6508495]\n",
      "loss: 0.528183  [4864000/6508495]\n",
      "loss: 0.529559  [4928000/6508495]\n",
      "loss: 0.546782  [4992000/6508495]\n",
      "loss: 0.577343  [5056000/6508495]\n",
      "loss: 0.531332  [5120000/6508495]\n",
      "loss: 0.512733  [5184000/6508495]\n",
      "loss: 0.542342  [5248000/6508495]\n",
      "loss: 0.548047  [5312000/6508495]\n",
      "loss: 0.533118  [5376000/6508495]\n",
      "loss: 0.523074  [5440000/6508495]\n",
      "loss: 0.528189  [5504000/6508495]\n",
      "loss: 0.536576  [5568000/6508495]\n",
      "loss: 0.637054  [5632000/6508495]\n",
      "loss: 0.568444  [5696000/6508495]\n",
      "loss: 0.516532  [5760000/6508495]\n",
      "loss: 0.513111  [5824000/6508495]\n",
      "loss: 0.606833  [5888000/6508495]\n",
      "loss: 0.524989  [5952000/6508495]\n",
      "loss: 0.517586  [6016000/6508495]\n",
      "loss: 0.515502  [6080000/6508495]\n",
      "loss: 0.511788  [6144000/6508495]\n",
      "loss: 0.541868  [6208000/6508495]\n",
      "loss: 0.524081  [6272000/6508495]\n",
      "loss: 0.543433  [6336000/6508495]\n",
      "loss: 0.540388  [6400000/6508495]\n",
      "loss: 0.509165  [6464000/6508495]\n",
      "Test set Error: \n",
      " Test Set Accuracy: 88.05811%, Avg Test Set loss: 0.552265 \n",
      "\n",
      "Training Set Error: \n",
      " Training Set Accuracy: 91.27099%, Avg Training Set loss: 0.537559 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.545381  [    0/6508495]\n",
      "loss: 0.524862  [64000/6508495]\n",
      "loss: 0.599676  [128000/6508495]\n",
      "loss: 0.514108  [192000/6508495]\n",
      "loss: 0.518362  [256000/6508495]\n",
      "loss: 0.523774  [320000/6508495]\n",
      "loss: 0.516467  [384000/6508495]\n",
      "loss: 0.520162  [448000/6508495]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     train_loop(train_dataloader, model, loss, optimizer)    \n\u001b[1;32m      6\u001b[0m     test_loop(test_dataloader, model, loss)\n\u001b[1;32m      7\u001b[0m     test_loop(train_dataloader, model, loss, test_set \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32m/home/aron/Artefact_Detection/trainModelRegression.ipynb Cell 8'\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpucluster.st.lab.au.dk/home/aron/Artefact_Detection/trainModelRegression.ipynb#ch0000007vscode-remote?line=5'>6</a>\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpucluster.st.lab.au.dk/home/aron/Artefact_Detection/trainModelRegression.ipynb#ch0000007vscode-remote?line=6'>7</a>\u001b[0m \u001b[39m# Compute prediction and loss\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpucluster.st.lab.au.dk/home/aron/Artefact_Detection/trainModelRegression.ipynb#ch0000007vscode-remote?line=7'>8</a>\u001b[0m pred \u001b[39m=\u001b[39m model(X)\u001b[39m# Reshape to 1 dimension if using binary classification, otherwise keep dimensions from model output\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgpucluster.st.lab.au.dk/home/aron/Artefact_Detection/trainModelRegression.ipynb#ch0000007vscode-remote?line=8'>9</a>\u001b[0m pred \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpucluster.st.lab.au.dk/home/aron/Artefact_Detection/trainModelRegression.ipynb#ch0000007vscode-remote?line=9'>10</a>\u001b[0m pred \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/aron/Artefact_Detection/trainModelRegression.ipynb Cell 6'\u001b[0m in \u001b[0;36mNeuralNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpucluster.st.lab.au.dk/home/aron/Artefact_Detection/trainModelRegression.ipynb#ch0000005vscode-remote?line=19'>20</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpucluster.st.lab.au.dk/home/aron/Artefact_Detection/trainModelRegression.ipynb#ch0000005vscode-remote?line=20'>21</a>\u001b[0m     \u001b[39m#x = self.flatten(x)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpucluster.st.lab.au.dk/home/aron/Artefact_Detection/trainModelRegression.ipynb#ch0000005vscode-remote?line=21'>22</a>\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear_relu_stack(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpucluster.st.lab.au.dk/home/aron/Artefact_Detection/trainModelRegression.ipynb#ch0000005vscode-remote?line=22'>23</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/container.py?line=140'>141</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/container.py?line=141'>142</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py:291\u001b[0m, in \u001b[0;36mSigmoid.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=289'>290</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///home/aron/.local/lib/python3.8/site-packages/torch/nn/modules/activation.py?line=290'>291</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49msigmoid(\u001b[39minput\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    \n",
    "    train_loop(train_dataloader, model, loss, optimizer)    \n",
    "    test_loop(test_dataloader, model, loss)\n",
    "    test_loop(train_dataloader, model, loss, test_set = False)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d334debdbe0c15d06a22e1a5d8333410545070b029a2d558077d9bd2658c7aed"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
