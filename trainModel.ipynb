{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install neptune-client torch torchvision\n",
    "#!pip install simplejson\n",
    "#!pip install torchinfo\n",
    "#!pip install neptune-notebooks\n",
    "#!jupyter nbextension enable --py neptune-notebooks\n",
    "#!python3 -m pip install torch\n",
    "#!python3 -V\n",
    "#!which python3\n",
    "#!conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda-2020.11/bin/python3\n",
      "Classification dataset version: mar-30-1\n"
     ]
    }
   ],
   "source": [
    "!which python3\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import EEGDataset\n",
    "from torch.utils.data import random_split\n",
    "import neptune.new as neptune\n",
    "from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set\n",
      "../data/study_1A_mat_simple/S_01/night_1/artefact_annotations.npy\n",
      "../data/study_1A_mat_simple/S_01/night_1/EEG_raw_250hz_unfiltered.npy\n",
      "../data/study_1A_mat_simple/S_01/night_2/artefact_annotations.npy\n",
      "../data/study_1A_mat_simple/S_01/night_2/EEG_raw_250hz_unfiltered.npy\n",
      "../data/study_1A_mat_simple/S_01/night_3/artefact_annotations.npy\n",
      "../data/study_1A_mat_simple/S_01/night_3/EEG_raw_250hz_unfiltered.npy\n",
      "../data/study_1A_mat_simple/S_01/night_4/artefact_annotations.npy\n",
      "../data/study_1A_mat_simple/S_01/night_4/EEG_raw_250hz_unfiltered.npy\n",
      "../data/study_1A_mat_simple/S_02/night_1/artefact_annotations.npy\n",
      "../data/study_1A_mat_simple/S_02/night_1/EEG_raw_250hz_unfiltered.npy\n",
      "../data/study_1A_mat_simple/S_02/night_2/artefact_annotations.npy\n",
      "../data/study_1A_mat_simple/S_02/night_2/EEG_raw_250hz_unfiltered.npy\n",
      "../data/study_1A_mat_simple/S_02/night_3/artefact_annotations.npy\n",
      "../data/study_1A_mat_simple/S_02/night_3/EEG_raw_250hz_unfiltered.npy\n",
      "../data/study_1A_mat_simple/S_02/night_4/artefact_annotations.npy\n",
      "Lables for night 0 loaded\n",
      "../data/study_1A_mat_simple/S_02/night_4/EEG_raw_250hz_unfiltered.npy\n",
      "Night 0 data loaded\n",
      "../data/study_1A_mat_simple/S_03/night_1/artefact_annotations.npy\n",
      "Lables for night 1 loaded\n",
      "../data/study_1A_mat_simple/S_03/night_1/EEG_raw_250hz_unfiltered.npy\n",
      "Night 1 data loaded\n",
      "Training set\n",
      "../data/study_1A_mat_simple/S_01/night_1/artefact_annotations.npy\n",
      "Lables for night 0 loaded\n",
      "../data/study_1A_mat_simple/S_01/night_1/EEG_raw_250hz_unfiltered.npy\n",
      "Night 0 data loaded\n",
      "../data/study_1A_mat_simple/S_01/night_2/artefact_annotations.npy\n",
      "Lables for night 1 loaded\n",
      "../data/study_1A_mat_simple/S_01/night_2/EEG_raw_250hz_unfiltered.npy\n",
      "Night 1 data loaded\n",
      "../data/study_1A_mat_simple/S_01/night_3/artefact_annotations.npy\n",
      "Lables for night 2 loaded\n",
      "../data/study_1A_mat_simple/S_01/night_3/EEG_raw_250hz_unfiltered.npy\n",
      "Night 2 data loaded\n",
      "../data/study_1A_mat_simple/S_01/night_4/artefact_annotations.npy\n",
      "Lables for night 3 loaded\n",
      "../data/study_1A_mat_simple/S_01/night_4/EEG_raw_250hz_unfiltered.npy\n",
      "Night 3 data loaded\n",
      "../data/study_1A_mat_simple/S_02/night_1/artefact_annotations.npy\n",
      "Lables for night 4 loaded\n",
      "../data/study_1A_mat_simple/S_02/night_1/EEG_raw_250hz_unfiltered.npy\n",
      "Night 4 data loaded\n",
      "../data/study_1A_mat_simple/S_02/night_2/artefact_annotations.npy\n",
      "Lables for night 5 loaded\n",
      "../data/study_1A_mat_simple/S_02/night_2/EEG_raw_250hz_unfiltered.npy\n",
      "Night 5 data loaded\n",
      "../data/study_1A_mat_simple/S_02/night_3/artefact_annotations.npy\n",
      "Lables for night 6 loaded\n",
      "../data/study_1A_mat_simple/S_02/night_3/EEG_raw_250hz_unfiltered.npy\n",
      "Night 6 data loaded\n"
     ]
    }
   ],
   "source": [
    "# load in the dataset\n",
    "\n",
    "#raw_data_dir = '//uni.au.dk/dfs/Tech_EarEEG/Students/RD2022_Artefact_AkselStark/data/1A/study_1A_mat_simple'\n",
    "raw_data_dir = '../data'\n",
    "\n",
    "trainingNights = 7\n",
    "testNights = 2\n",
    "segLength = 750\n",
    "\n",
    "print(\"Test set\")\n",
    "ds2 = EEGDataset(raw_data_dir,testNights, segLength, skips = trainingNights)\n",
    "\n",
    "print(\"Training set\")\n",
    "ds1 = EEGDataset(raw_data_dir,trainingNights, segLength, skips = 0) #Instantiate a dataset using the directory of data, amount of night to include and amount of samples in a segment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_ratio:0.9875976387812295\n"
     ]
    }
   ],
   "source": [
    "#Calculate class imbalance\n",
    "balancing_dataloader = DataLoader(ds1, drop_last = True) # Drop_last, to avoid incomplete batches, which won't work with weighted loss\n",
    "artefacts = 0\n",
    "good_samples = 0\n",
    "for batch, (X, y) in enumerate(balancing_dataloader):\n",
    "    if y == 1:\n",
    "        artefacts += 1\n",
    "    else:\n",
    "        good_samples += 1\n",
    "    if batch > 100000:\n",
    "        break\n",
    "\n",
    "class_ratio = good_samples/artefacts\n",
    "print(f\"class_ratio:{class_ratio}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' #Check for cuda \n",
    "print(f'Using {device} device')\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        #self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.BatchNorm1d(segLength+2),\n",
    "            nn.Linear(segLength+2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid(), \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model.to(device)\n",
    "\n",
    "#pos_weight: amount of positive examples compared to negative examples. Calculate as: negative_examples/positive_examples\n",
    "loss = nn.BCEWithLogitsLoss(pos_weight = class_ratio*torch.ones([batch_size]).to(device)) \n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)# Reshape to 1 dimension if using binary classification, otherwise keep dimensions from model output\n",
    "        pred = pred.reshape(-1)\n",
    "        pred = pred.to(device)\n",
    "        yFloat = y.type(torch.FloatTensor).to(device)\n",
    "        \n",
    "        loss = loss_fn(pred, yFloat)\n",
    "        \n",
    "\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Neptune logging\n",
    "        run[\"training/batch/loss\"].log(loss)\n",
    "        \n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \n",
    "        if batch % 10000 == 0:\n",
    "            print(f\"Predicted values: \\n{pred}\")\n",
    "            print(f\"Actual values: \\n{yFloat}\")\n",
    "            print(f\"Difference: \\n{(yFloat-pred)}\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader_test, model, loss_fn, test_set = True):\n",
    "    size = len(dataloader_test.dataset)\n",
    "    num_batches = len(dataloader_test)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader_test:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X).reshape(-1).to(device) # Reshape to 1 dimension if using binary classification, otherwise keep dimensions from model output\n",
    "            test_loss += loss_fn(pred, y.type(torch.FloatTensor).to(device)).item()\n",
    "            correct += (pred.round() == y).type(torch.float).sum().item()\n",
    "            \n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    \n",
    "    \n",
    "    \n",
    "    if test_set:\n",
    "        print(f\"Test set Error: \\n Test Set Accuracy: {(100*correct):>0.5f}%, Avg Test Set loss: {test_loss:>8f} \\n\")\n",
    "        \n",
    "        # Neptune logging\n",
    "        run[\"testing/batch/test_loss\"].log(test_loss)\n",
    "        run[\"testing/batch/test_Acc\"].log(100*correct)\n",
    "    \n",
    "    else:\n",
    "        print(f\"Training Set Error: \\n Training Set Accuracy: {(100*correct):>0.5f}%, Avg Training Set loss: {test_loss:>8f} \\n\")\n",
    "        \n",
    "        # Neptune logging\n",
    "        run[\"testing/batch/training_loss\"].log(test_loss)\n",
    "        run[\"testing/batch/training_Acc\"].log(100*correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test data (Commented out: Changed to loading different dataset class instances)\n",
    "#trainSamples = int(ds1.__len__()*0.7)\n",
    "#testSamples = int(ds1.__len__() - trainSamples)\n",
    "#training_data, test_data = random_split(ds1, (trainSamples,testSamples), generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "#train_dataloader = DataLoader(training_data, batch_size=64, drop_last = True) # Drop_last, to avoid incomplete batches, which won't work with weighted loss\n",
    "#test_dataloader = DataLoader(test_data, batch_size=64, drop_last = True) # Drop_last, to avoid incomplete batches, which won't work with weighted loss\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(ds1, batch_size=64, drop_last = True) # Drop_last, to avoid incomplete batches, which won't work with weighted loss\n",
    "test_dataloader = DataLoader(ds2, batch_size=64, drop_last = True) # Drop_last, to avoid incomplete batches, which won't work with weighted loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/aksel.s.madsen/artefact-detection/e/AR-41\n",
      "Remember to stop your run once youâ€™ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "# Initialize neptune\n",
    "run = neptune.init(\n",
    "    project=\"aksel.s.madsen/artefact-detection\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIxYTA4NzcxMy1lYmQ2LTQ3NTctYjRhNC02Mzk1NjdjMWM0NmYifQ==\",\n",
    "    source_files=[\"trainModel.ipynb\", \"dataset.py\"]\n",
    ")  # Credentials\n",
    "\n",
    "\n",
    "run['config/dataset/size'] = trainingNights # dict() object\n",
    "run['config/model'] = type(model).__name__\n",
    "run['config/modelSummary'] = summary(model, input_size=(batch_size, segLength + 2))\n",
    "run['config/optimizer'] = type(optimizer).__name__\n",
    "run['config/batch_size'] = batch_size\n",
    "run['config/test_night'] = testNights\n",
    "run['config/learning_rate'] = learning_rate\n",
    "run['config/segLength'] = segLength\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.663429  [    0/1650495]\n",
      "Predicted values: \n",
      "tensor([0.5219, 0.5051, 0.5121, 0.5049, 0.5811, 0.5047, 0.5108, 0.5041, 0.5165,\n",
      "        0.5036, 0.5039, 0.5044, 0.5327, 0.5039, 0.5062, 0.5041, 0.5049, 0.5047,\n",
      "        0.5137, 0.5048, 0.5084, 0.5043, 0.5021, 0.5046, 0.5254, 0.5038, 0.5106,\n",
      "        0.5039, 0.5042, 0.5049, 0.5074, 0.5043, 0.5054, 0.5044, 0.5030, 0.5045,\n",
      "        0.5234, 0.5059, 0.5221, 0.5056, 0.5218, 0.5038, 0.5057, 0.5075, 0.5138,\n",
      "        0.5039, 0.5039, 0.5044, 0.5051, 0.5033, 0.5087, 0.5082, 0.5175, 0.5037,\n",
      "        0.5043, 0.5041, 0.5574, 0.5044, 0.5512, 0.5052, 0.5514, 0.5044, 0.5044,\n",
      "        0.5044], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "Actual values: \n",
      "tensor([1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 1., 0., 1., 1., 1., 0., 1., 0.], device='cuda:0')\n",
      "Difference: \n",
      "tensor([ 0.4781, -0.5051,  0.4879, -0.5049,  0.4189, -0.5047,  0.4892, -0.5041,\n",
      "         0.4835, -0.5036,  0.4961, -0.5044,  0.4673, -0.5039,  0.4938, -0.5041,\n",
      "         0.4951, -0.5047,  0.4863, -0.5048,  0.4916, -0.5043,  0.4979, -0.5046,\n",
      "         0.4746, -0.5038,  0.4894, -0.5039,  0.4958, -0.5049,  0.4926, -0.5043,\n",
      "         0.4946, -0.5044,  0.4970, -0.5045,  0.4766,  0.4941,  0.4779, -0.5056,\n",
      "         0.4782, -0.5038,  0.4943,  0.4925,  0.4862,  0.4961,  0.4961, -0.5044,\n",
      "         0.4949,  0.4967,  0.4913,  0.4918,  0.4825,  0.4963,  0.4957, -0.5041,\n",
      "         0.4426, -0.5044,  0.4488,  0.4948,  0.4486, -0.5044,  0.4956, -0.5044],\n",
      "       device='cuda:0', grad_fn=<SubBackward0>)\n",
      "loss: 0.624860  [64000/1650495]\n",
      "loss: 0.660057  [128000/1650495]\n",
      "loss: 0.546350  [192000/1650495]\n",
      "loss: 0.535709  [256000/1650495]\n",
      "loss: 0.523734  [320000/1650495]\n",
      "loss: 0.523273  [384000/1650495]\n",
      "loss: 0.578533  [448000/1650495]\n",
      "loss: 0.640893  [512000/1650495]\n",
      "loss: 0.512339  [576000/1650495]\n",
      "loss: 0.811328  [640000/1650495]\n",
      "Predicted values: \n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9979, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "Actual values: \n",
      "tensor([1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 0., 1., 0., 1., 0.], device='cuda:0')\n",
      "Difference: \n",
      "tensor([ 0.0000e+00, -1.0000e+00,  0.0000e+00, -1.0000e+00,  0.0000e+00,\n",
      "        -1.0000e+00,  0.0000e+00, -1.0000e+00,  0.0000e+00, -1.0000e+00,\n",
      "         0.0000e+00, -1.0000e+00,  0.0000e+00, -1.0000e+00,  0.0000e+00,\n",
      "        -1.0000e+00,  2.0884e-03, -1.0000e+00,  0.0000e+00, -1.0000e+00,\n",
      "         0.0000e+00, -1.0000e+00,  0.0000e+00, -1.0000e+00,  0.0000e+00,\n",
      "        -1.0000e+00,  0.0000e+00, -1.0000e+00,  0.0000e+00, -1.0000e+00,\n",
      "         2.3842e-07, -1.0000e+00,  1.1921e-07, -1.0000e+00,  0.0000e+00,\n",
      "        -1.0000e+00,  0.0000e+00, -1.0000e+00,  0.0000e+00, -1.0000e+00,\n",
      "         0.0000e+00, -1.0000e+00,  0.0000e+00, -1.0000e+00,  0.0000e+00,\n",
      "        -1.0000e+00,  0.0000e+00, -1.0000e+00,  0.0000e+00, -1.0000e+00,\n",
      "         0.0000e+00, -1.0000e+00,  0.0000e+00, -1.0000e+00,  0.0000e+00,\n",
      "        -1.0000e+00,  0.0000e+00, -1.0000e+00,  0.0000e+00, -1.0000e+00,\n",
      "         0.0000e+00, -1.0000e+00,  0.0000e+00, -1.0000e+00], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>)\n",
      "loss: 0.527865  [704000/1650495]\n",
      "loss: 0.507291  [768000/1650495]\n",
      "loss: 0.513110  [832000/1650495]\n",
      "loss: 0.530523  [896000/1650495]\n",
      "loss: 0.509110  [960000/1650495]\n",
      "loss: 0.524973  [1024000/1650495]\n",
      "loss: 0.512685  [1088000/1650495]\n",
      "loss: 0.501333  [1152000/1650495]\n",
      "loss: 0.530542  [1216000/1650495]\n",
      "loss: 0.521600  [1280000/1650495]\n",
      "Predicted values: \n",
      "tensor([1.0000e+00, 1.4175e-05, 1.0000e+00, 1.5862e-05, 4.5450e-01, 2.1684e-05,\n",
      "        1.0000e+00, 2.3211e-05, 5.6618e-04, 1.6061e-05, 1.0000e+00, 2.5825e-05,\n",
      "        1.0000e+00, 4.8483e-05, 1.0000e+00, 2.5083e-05, 9.9987e-01, 2.1399e-05,\n",
      "        9.9985e-01, 1.7623e-05, 9.9994e-01, 2.4447e-05, 1.0000e+00, 6.4027e-05,\n",
      "        1.0000e+00, 2.0577e-05, 1.0000e+00, 3.0882e-05, 1.0000e+00, 1.7614e-05,\n",
      "        1.0000e+00, 1.8651e-05, 1.0000e+00, 1.8364e-05, 1.0000e+00, 1.9798e-05,\n",
      "        1.0000e+00, 2.0680e-05, 1.0000e+00, 1.6275e-05, 1.3892e-04, 6.2473e-05,\n",
      "        1.0000e+00, 1.8421e-05, 2.9547e-04, 9.5982e-05, 1.0000e+00, 1.8194e-05,\n",
      "        1.0000e+00, 2.1456e-05, 1.0000e+00, 2.6927e-05, 1.0000e+00, 2.7577e-05,\n",
      "        1.0000e+00, 2.6437e-05, 1.0000e+00, 2.5003e-05, 1.0000e+00, 5.5454e-05,\n",
      "        1.0000e+00, 4.6225e-05, 1.0000e+00, 6.8878e-05], device='cuda:0',\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n",
      "Actual values: \n",
      "tensor([1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 0., 1., 0., 1., 0.], device='cuda:0')\n",
      "Difference: \n",
      "tensor([ 0.0000e+00, -1.4175e-05,  0.0000e+00, -1.5862e-05,  5.4550e-01,\n",
      "        -2.1684e-05,  0.0000e+00, -2.3211e-05,  9.9943e-01, -1.6061e-05,\n",
      "         0.0000e+00, -2.5825e-05,  0.0000e+00, -4.8483e-05,  1.1921e-07,\n",
      "        -2.5083e-05,  1.2827e-04, -2.1399e-05,  1.4961e-04, -1.7623e-05,\n",
      "         5.7697e-05, -2.4447e-05,  0.0000e+00, -6.4027e-05,  0.0000e+00,\n",
      "        -2.0577e-05,  0.0000e+00, -3.0882e-05,  0.0000e+00, -1.7614e-05,\n",
      "         0.0000e+00, -1.8651e-05,  0.0000e+00, -1.8364e-05,  0.0000e+00,\n",
      "        -1.9798e-05,  0.0000e+00, -2.0680e-05,  0.0000e+00, -1.6275e-05,\n",
      "         9.9986e-01, -6.2473e-05,  0.0000e+00, -1.8421e-05,  9.9970e-01,\n",
      "        -9.5982e-05,  0.0000e+00, -1.8194e-05,  0.0000e+00, -2.1456e-05,\n",
      "         0.0000e+00, -2.6927e-05,  0.0000e+00, -2.7577e-05,  0.0000e+00,\n",
      "        -2.6437e-05,  0.0000e+00, -2.5003e-05,  0.0000e+00, -5.5454e-05,\n",
      "         0.0000e+00, -4.6225e-05,  0.0000e+00, -6.8878e-05], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>)\n",
      "loss: 0.511194  [1344000/1650495]\n",
      "loss: 0.510322  [1408000/1650495]\n",
      "loss: 0.507141  [1472000/1650495]\n",
      "loss: 0.850777  [1536000/1650495]\n",
      "loss: 0.501325  [1600000/1650495]\n",
      "Test set Error: \n",
      " Test Set Accuracy: 84.11834%, Avg Test Set loss: 0.573358 \n",
      "\n",
      "Training Set Error: \n",
      " Training Set Accuracy: 92.01446%, Avg Training Set loss: 0.535168 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.524054  [    0/1650495]\n",
      "Predicted values: \n",
      "tensor([1.0000e+00, 1.3222e-05, 1.0000e+00, 3.5889e-05, 7.7666e-01, 3.6483e-05,\n",
      "        9.3240e-03, 1.5834e-05, 1.0000e+00, 1.3346e-05, 1.0000e+00, 1.0388e-05,\n",
      "        1.0000e+00, 1.0577e-05, 1.0000e+00, 1.1626e-05, 1.0000e+00, 2.3800e-05,\n",
      "        1.0000e+00, 2.5518e-05, 1.0000e+00, 1.2989e-05, 3.1985e-03, 1.5959e-05,\n",
      "        1.0000e+00, 1.7883e-05, 6.5923e-01, 9.5079e-06, 1.0000e+00, 1.6583e-05,\n",
      "        1.0000e+00, 2.0882e-05, 6.9291e-05, 1.8399e-05, 5.7719e-04, 2.0901e-05,\n",
      "        1.0000e+00, 4.4443e-05, 9.8550e-01, 1.6216e-04, 1.0000e+00, 1.6979e-05,\n",
      "        9.9777e-01, 9.9997e-01, 1.0000e+00, 9.7704e-01, 1.0000e+00, 2.8552e-05,\n",
      "        9.9941e-01, 2.3099e-05, 1.2462e-02, 1.0000e+00, 1.0000e+00, 1.7924e-04,\n",
      "        2.9715e-01, 1.1592e-05, 2.2284e-02, 1.5482e-05, 1.0000e+00, 1.9966e-05,\n",
      "        1.0000e+00, 1.5368e-05, 1.0000e+00, 1.1460e-05], device='cuda:0',\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n",
      "Actual values: \n",
      "tensor([1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 1., 0., 1., 1., 1., 0., 1., 0.], device='cuda:0')\n",
      "Difference: \n",
      "tensor([ 0.0000e+00, -1.3222e-05,  0.0000e+00, -3.5889e-05,  2.2334e-01,\n",
      "        -3.6483e-05,  9.9068e-01, -1.5834e-05,  0.0000e+00, -1.3346e-05,\n",
      "         0.0000e+00, -1.0388e-05,  0.0000e+00, -1.0577e-05,  0.0000e+00,\n",
      "        -1.1626e-05,  0.0000e+00, -2.3800e-05,  0.0000e+00, -2.5518e-05,\n",
      "         1.1921e-07, -1.2989e-05,  9.9680e-01, -1.5959e-05,  0.0000e+00,\n",
      "        -1.7883e-05,  3.4077e-01, -9.5079e-06,  0.0000e+00, -1.6583e-05,\n",
      "         0.0000e+00, -2.0882e-05,  9.9993e-01, -1.8399e-05,  9.9942e-01,\n",
      "        -2.0901e-05,  0.0000e+00,  9.9996e-01,  1.4499e-02, -1.6216e-04,\n",
      "         0.0000e+00, -1.6979e-05,  2.2309e-03,  2.5272e-05,  0.0000e+00,\n",
      "         2.2958e-02,  0.0000e+00, -2.8552e-05,  5.9485e-04,  9.9998e-01,\n",
      "         9.8754e-01,  1.6689e-06,  0.0000e+00,  9.9982e-01,  7.0285e-01,\n",
      "        -1.1592e-05,  9.7772e-01, -1.5482e-05,  0.0000e+00,  9.9998e-01,\n",
      "         0.0000e+00, -1.5368e-05,  1.0729e-06, -1.1460e-05], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.604051  [64000/1650495]\n",
      "loss: 0.565192  [128000/1650495]\n",
      "loss: 0.550774  [192000/1650495]\n",
      "loss: 0.530571  [256000/1650495]\n",
      "loss: 0.511123  [320000/1650495]\n",
      "loss: 0.503504  [384000/1650495]\n",
      "loss: 0.553626  [448000/1650495]\n",
      "loss: 0.588648  [512000/1650495]\n",
      "loss: 0.501356  [576000/1650495]\n",
      "loss: 0.809809  [640000/1650495]\n",
      "Predicted values: \n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        0.9694, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 0.8536, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000], device='cuda:0', grad_fn=<ReshapeAliasBackward0>)\n",
      "Actual values: \n",
      "tensor([1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 0., 1., 0., 1., 0.], device='cuda:0')\n",
      "Difference: \n",
      "tensor([ 0.0000e+00, -1.0000e+00,  0.0000e+00, -1.0000e+00,  0.0000e+00,\n",
      "        -1.0000e+00,  1.0490e-05, -1.0000e+00,  0.0000e+00, -1.0000e+00,\n",
      "         0.0000e+00, -1.0000e+00,  0.0000e+00, -1.0000e+00,  0.0000e+00,\n",
      "        -1.0000e+00,  0.0000e+00, -1.0000e+00,  0.0000e+00, -1.0000e+00,\n",
      "         0.0000e+00, -1.0000e+00,  0.0000e+00, -1.0000e+00,  0.0000e+00,\n",
      "        -1.0000e+00,  0.0000e+00, -1.0000e+00,  3.4571e-06, -1.0000e+00,\n",
      "         0.0000e+00, -1.0000e+00,  0.0000e+00, -1.0000e+00,  0.0000e+00,\n",
      "        -1.0000e+00,  3.0551e-02, -1.0000e+00,  0.0000e+00, -1.0000e+00,\n",
      "         0.0000e+00, -1.0000e+00,  0.0000e+00, -1.0000e+00,  0.0000e+00,\n",
      "        -1.0000e+00,  0.0000e+00, -8.5361e-01,  0.0000e+00, -1.0000e+00,\n",
      "         0.0000e+00, -1.0000e+00,  0.0000e+00, -1.0000e+00,  0.0000e+00,\n",
      "        -1.0000e+00,  0.0000e+00, -1.0000e+00,  0.0000e+00, -1.0000e+00,\n",
      "         1.0729e-06, -1.0000e+00,  0.0000e+00, -1.0000e+00], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>)\n",
      "loss: 0.512520  [704000/1650495]\n",
      "loss: 0.508016  [768000/1650495]\n",
      "loss: 0.527731  [832000/1650495]\n",
      "loss: 0.520427  [896000/1650495]\n",
      "loss: 0.513876  [960000/1650495]\n",
      "loss: 0.520638  [1024000/1650495]\n",
      "loss: 0.523806  [1088000/1650495]\n",
      "loss: 0.501597  [1152000/1650495]\n",
      "loss: 0.536194  [1216000/1650495]\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    \n",
    "    train_loop(train_dataloader, model, loss, optimizer)    \n",
    "    test_loop(test_dataloader, model, loss)\n",
    "    test_loop(train_dataloader, model, loss, test_set = False)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model, and inspect the errors\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "\n",
    "print(f\"../trained_models/model_{now.strftime('%m_%d_%Y_%H_%M_%S')}\")\n",
    "\n",
    "model_scripted = torch.jit.script(model) # Export to TorchScript\n",
    "model_scripted.save(f\"../trained_models/model_{now.strftime('%m_%d_%Y_%H_%M_%S')}\") # Save\n",
    "\n",
    "print('debug')\n",
    "#randChannel = \n",
    "#testData = ds2\n",
    "#self.labels[channel, start : start + self.sectionLength]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = torch.load(\"../trained_models/model_03_21_2022_23_19_06\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.stop() # Stop the neptune logging run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d334debdbe0c15d06a22e1a5d8333410545070b029a2d558077d9bd2658c7aed"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "neptune": {
   "notebookId": "95f2132b-e8b8-43da-ae1c-99722613d84e",
   "projectVersion": 2
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
