{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aksel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\Aksel\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import EEGDataset\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//uni.au.dk/dfs/Tech_EarEEG/Students/RD2022_Artefact_AkselStark/data/1A/study_1A_mat_simple\\S_01\\night_1\\artefact_annotations.npy\n",
      "Lables for night 0 loaded\n",
      "//uni.au.dk/dfs/Tech_EarEEG/Students/RD2022_Artefact_AkselStark/data/1A/study_1A_mat_simple\\S_01\\night_1\\EEG_raw_250hz.npy\n",
      "Night 0 data loaded\n",
      "//uni.au.dk/dfs/Tech_EarEEG/Students/RD2022_Artefact_AkselStark/data/1A/study_1A_mat_simple\\S_01\\night_2\\artefact_annotations.npy\n",
      "Lables for night 1 loaded\n",
      "//uni.au.dk/dfs/Tech_EarEEG/Students/RD2022_Artefact_AkselStark/data/1A/study_1A_mat_simple\\S_01\\night_2\\EEG_raw_250hz.npy\n",
      "Night 1 data loaded\n"
     ]
    }
   ],
   "source": [
    "# load in the dataset\n",
    "\n",
    "raw_data_dir = '//uni.au.dk/dfs/Tech_EarEEG/Students/RD2022_Artefact_AkselStark/data/1A/study_1A_mat_simple'\n",
    "\n",
    "ds1 = EEGDataset(raw_data_dir,2, 250) #Instantiate a dataset using the directory of data, amount of night to include and amount of samples in a segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' #Check for cuda \n",
    "print(f'Using {device} device')\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        #self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(250, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "model = NeuralNetwork()\n",
    "\n",
    "#Weighting artefacts 10x\n",
    "#loss = nn.BCELoss(weight = torch.tensor([0.1,0.9]))\n",
    "\n",
    "#weights = [1,10]\n",
    "#class_weights = torch.FloatTensor(weights)#.cuda()\n",
    "\n",
    "#loss = nn.CrossEntropyLoss(weight=class_weights) # Using cross entropy and not binary cross entropy because weight implementation is better :/\n",
    "#loss = nn.CrossEntropyLoss(class_weights)\n",
    "\n",
    "#pos_weight: amount of positive examples compared to negative examples. Calculate as: negative_examples/positive_examples\n",
    "loss = nn.BCEWithLogitsLoss(pos_weight = 20*torch.ones([batch_size])) \n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X).reshape(-1)# Reshape to 1 dimension if using binary classification, otherwise keep dimensions from model output\n",
    "        loss = loss_fn(pred, y.type(torch.FloatTensor))\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X).reshape(-1) # Reshape to 1 dimension if using binary classification, otherwise keep dimensions from model output\n",
    "            test_loss += loss_fn(pred, y.type(torch.FloatTensor)).item()\n",
    "            correct += (pred.round() == y).type(torch.float).sum().item()\n",
    "            \n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test data\n",
    "trainSamples = int(ds1.__len__()*0.7)\n",
    "testSamples = int(ds1.__len__() - trainSamples)\n",
    "training_data, test_data = random_split(ds1, (trainSamples,testSamples), generator=torch.Generator().manual_seed(42))\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.011110  [    0/906674]\n",
      "loss: 1.516259  [ 6400/906674]\n",
      "loss: 1.533805  [12800/906674]\n",
      "loss: 1.735191  [19200/906674]\n",
      "loss: 1.525024  [25600/906674]\n",
      "loss: 1.525022  [32000/906674]\n",
      "loss: 1.739555  [38400/906674]\n",
      "loss: 1.332367  [44800/906674]\n",
      "loss: 0.907675  [51200/906674]\n",
      "loss: 1.680017  [57600/906674]\n",
      "loss: 1.327969  [64000/906674]\n",
      "loss: 1.327968  [70400/906674]\n",
      "loss: 1.735143  [76800/906674]\n",
      "loss: 1.524995  [83200/906674]\n",
      "loss: 0.903292  [89600/906674]\n",
      "loss: 1.113432  [96000/906674]\n",
      "loss: 1.332296  [102400/906674]\n",
      "loss: 1.800310  [108800/906674]\n",
      "loss: 1.122139  [115200/906674]\n",
      "loss: 1.323555  [121600/906674]\n",
      "loss: 0.706215  [128000/906674]\n",
      "loss: 1.456937  [134400/906674]\n",
      "loss: 1.730742  [140800/906674]\n",
      "loss: 1.940869  [147200/906674]\n",
      "loss: 1.743788  [153600/906674]\n",
      "loss: 1.520608  [160000/906674]\n",
      "loss: 1.323542  [166400/906674]\n",
      "loss: 1.109049  [172800/906674]\n",
      "loss: 1.735069  [179200/906674]\n",
      "loss: 1.113387  [185600/906674]\n",
      "loss: 1.327841  [192000/906674]\n",
      "loss: 1.310482  [198400/906674]\n",
      "loss: 1.457097  [204800/906674]\n",
      "loss: 1.287465  [211200/906674]\n",
      "loss: 1.098003  [217600/906674]\n",
      "loss: 1.369716  [224000/906674]\n",
      "loss: 1.247398  [230400/906674]\n",
      "loss: 1.205271  [236800/906674]\n",
      "loss: 1.467415  [243200/906674]\n",
      "loss: 1.397145  [249600/906674]\n",
      "loss: 1.343364  [256000/906674]\n",
      "loss: 1.433750  [262400/906674]\n",
      "loss: 1.561579  [268800/906674]\n",
      "loss: 1.291305  [275200/906674]\n",
      "loss: 1.381086  [281600/906674]\n",
      "loss: 1.339001  [288000/906674]\n",
      "loss: 1.083897  [294400/906674]\n",
      "loss: 1.322516  [300800/906674]\n",
      "loss: 1.228945  [307200/906674]\n",
      "loss: 1.326722  [313600/906674]\n",
      "loss: 1.142904  [320000/906674]\n",
      "loss: 1.171830  [326400/906674]\n",
      "loss: 1.035419  [332800/906674]\n",
      "loss: 1.095131  [339200/906674]\n",
      "loss: 0.900945  [345600/906674]\n",
      "loss: 1.217687  [352000/906674]\n",
      "loss: 1.041822  [358400/906674]\n",
      "loss: 1.636141  [364800/906674]\n",
      "loss: 1.306881  [371200/906674]\n",
      "loss: 2.074035  [377600/906674]\n",
      "loss: 1.205195  [384000/906674]\n",
      "loss: 1.348567  [390400/906674]\n",
      "loss: 1.097815  [396800/906674]\n",
      "loss: 1.198219  [403200/906674]\n",
      "loss: 1.263178  [409600/906674]\n",
      "loss: 1.097795  [416000/906674]\n",
      "loss: 1.147179  [422400/906674]\n",
      "loss: 1.112720  [428800/906674]\n",
      "loss: 0.968746  [435200/906674]\n",
      "loss: 1.470423  [441600/906674]\n",
      "loss: 1.523864  [448000/906674]\n",
      "loss: 1.313262  [454400/906674]\n",
      "loss: 1.689365  [460800/906674]\n",
      "loss: 1.147171  [467200/906674]\n",
      "loss: 1.376622  [473600/906674]\n",
      "loss: 1.116999  [480000/906674]\n",
      "loss: 1.271272  [486400/906674]\n",
      "loss: 1.137467  [492800/906674]\n",
      "loss: 1.009417  [499200/906674]\n",
      "loss: 1.142862  [505600/906674]\n",
      "loss: 0.934261  [512000/906674]\n",
      "loss: 1.290099  [518400/906674]\n",
      "loss: 1.258849  [524800/906674]\n",
      "loss: 1.544722  [531200/906674]\n",
      "loss: 1.713438  [537600/906674]\n",
      "loss: 1.326669  [544000/906674]\n",
      "loss: 1.322700  [550400/906674]\n",
      "loss: 1.098676  [556800/906674]\n",
      "loss: 1.358315  [563200/906674]\n",
      "loss: 1.068593  [569600/906674]\n",
      "loss: 0.886312  [576000/906674]\n",
      "loss: 1.504937  [582400/906674]\n",
      "loss: 1.021285  [588800/906674]\n",
      "loss: 1.499102  [595200/906674]\n",
      "loss: 1.789651  [601600/906674]\n",
      "loss: 1.265809  [608000/906674]\n",
      "loss: 1.441106  [614400/906674]\n",
      "loss: 1.329231  [620800/906674]\n",
      "loss: 1.399498  [627200/906674]\n",
      "loss: 1.300161  [633600/906674]\n",
      "loss: 1.127702  [640000/906674]\n",
      "loss: 1.270379  [646400/906674]\n",
      "loss: 1.347412  [652800/906674]\n",
      "loss: 1.291076  [659200/906674]\n",
      "loss: 1.194179  [665600/906674]\n",
      "loss: 1.102865  [672000/906674]\n",
      "loss: 1.367975  [678400/906674]\n",
      "loss: 1.319528  [684800/906674]\n",
      "loss: 1.225639  [691200/906674]\n",
      "loss: 1.155376  [697600/906674]\n",
      "loss: 1.362517  [704000/906674]\n",
      "loss: 1.097381  [710400/906674]\n",
      "loss: 1.236689  [716800/906674]\n",
      "loss: 1.321726  [723200/906674]\n",
      "loss: 1.073893  [729600/906674]\n",
      "loss: 1.417452  [736000/906674]\n",
      "loss: 1.296319  [742400/906674]\n",
      "loss: 1.122184  [748800/906674]\n",
      "loss: 1.644542  [755200/906674]\n",
      "loss: 1.185776  [761600/906674]\n",
      "loss: 1.029635  [768000/906674]\n",
      "loss: 1.398514  [774400/906674]\n",
      "loss: 1.046039  [780800/906674]\n",
      "loss: 1.186882  [787200/906674]\n",
      "loss: 1.674549  [793600/906674]\n",
      "loss: 1.652585  [800000/906674]\n",
      "loss: 1.309806  [806400/906674]\n",
      "loss: 1.337599  [812800/906674]\n",
      "loss: 1.281082  [819200/906674]\n",
      "loss: 1.512996  [825600/906674]\n",
      "loss: 1.680336  [832000/906674]\n",
      "loss: 1.322402  [838400/906674]\n",
      "loss: 0.994996  [844800/906674]\n",
      "loss: 1.403966  [851200/906674]\n",
      "loss: 1.261665  [857600/906674]\n",
      "loss: 1.170401  [864000/906674]\n",
      "loss: 1.485198  [870400/906674]\n",
      "loss: 1.478418  [876800/906674]\n",
      "loss: 1.576582  [883200/906674]\n",
      "loss: 1.251929  [889600/906674]\n",
      "loss: 1.022443  [896000/906674]\n",
      "loss: 0.846859  [902400/906674]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (50) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25528\\155112318.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtrain_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mtest_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25528\\4009626372.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;31m# Compute prediction and loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# Reshape to 1 dimension if using binary classification, otherwise keep dimensions from model output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# Backpropagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m         return F.binary_cross_entropy_with_logits(input, target,\n\u001b[0m\u001b[0;32m    705\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   2980\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2981\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2982\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2984\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (50) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    \n",
    "    train_loop(train_dataloader, model, loss, optimizer)    \n",
    "    test_loop(test_dataloader, model, loss)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('debug')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d334debdbe0c15d06a22e1a5d8333410545070b029a2d558077d9bd2658c7aed"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
